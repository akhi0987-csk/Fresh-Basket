{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   InvoiceNo    498 non-null    object        \n",
      " 1   StockCode    498 non-null    object        \n",
      " 2   User ID      498 non-null    object        \n",
      " 3   Product ID   498 non-null    object        \n",
      " 4   Name         498 non-null    object        \n",
      " 5   Brand        498 non-null    object        \n",
      " 6   Price        498 non-null    float64       \n",
      " 7   Category     498 non-null    object        \n",
      " 8   SubCategory  498 non-null    object        \n",
      " 9   Quantity     498 non-null    float64       \n",
      " 10  Description  498 non-null    object        \n",
      " 11  Type         498 non-null    object        \n",
      " 12  Rating       498 non-null    int64         \n",
      " 13  InvoiceDate  498 non-null    datetime64[ns]\n",
      " 14  time stamp   498 non-null    int64         \n",
      " 15  Country      498 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(11)\n",
      "memory usage: 62.4+ KB\n",
      "None\n",
      "  InvoiceNo StockCode         User ID  Product ID  \\\n",
      "0    536365    85123A  A3EI9TX2A4MUSZ   594549507   \n",
      "1    536365     71053  A2UVFEZ7UDBRAA   594481813   \n",
      "2    536365    84406B  A2J3WVIQ5LO1O7   594481902   \n",
      "3    536365    84029G   AJ4QIAKKHW21N  089933623X   \n",
      "4    536365    84029E  A1PY7ODDSRIFGV   879393742   \n",
      "\n",
      "                                    Name       Brand  Price Category  \\\n",
      "0                     Aabad Cow Ghee Jar       Aabad  615.0  Grocery   \n",
      "1                   Aabad Cow Ghee Pouch       Aabad  599.0  Grocery   \n",
      "2                  Aabad Desi Ghee Pouch       Aabad  599.0  Grocery   \n",
      "3             Aashirvaad Multigrain Atta  Aashirvaad  337.0  Grocery   \n",
      "4  Aashirvaad Select Sharbati Wheat Atta  Aashirvaad  321.0  Grocery   \n",
      "\n",
      "        SubCategory  Quantity  \\\n",
      "0  Ghee & Vanaspati       1.0   \n",
      "1  Ghee & Vanaspati       1.0   \n",
      "2  Ghee & Vanaspati       1.0   \n",
      "3   Flours & Grains       5.0   \n",
      "4   Flours & Grains       5.0   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Ingredients Ghee (Milk Fat)StorageStore in a c...   \n",
      "1                       Ingredients  Ghee (Milk Fat)   \n",
      "2  Key Features- Strengthens the Immune system- G...   \n",
      "3  \\n- Looking to switch to healthier flour optio...   \n",
      "4  \\n- Made from the King of Wheat â€“ Sharbati, Aa...   \n",
      "\n",
      "                         Type  Rating         InvoiceDate  time stamp  \\\n",
      "0  Grocery > Ghee & Vanaspati       4 2010-12-01 08:26:00  1386720000   \n",
      "1  Grocery > Ghee & Vanaspati       5 2010-12-01 08:26:00  1368576000   \n",
      "2  Grocery > Ghee & Vanaspati       4 2010-12-01 08:26:00  1400284800   \n",
      "3   Grocery > Flours & Grains       1 2010-12-01 08:26:00  1312156800   \n",
      "4   Grocery > Flours & Grains       4 2010-12-01 08:26:00  1368057600   \n",
      "\n",
      "          Country  \n",
      "0  United Kingdom  \n",
      "1  United Kingdom  \n",
      "2  United Kingdom  \n",
      "3  United Kingdom  \n",
      "4  United Kingdom  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (update file path if needed)\n",
    "file_path = 'C:\\\\Users\\\\DASARI AKHILA\\\\OneDrive\\\\Documents\\\\CUSTOMER SUPPORT\\\\Backend\\\\Files\\\\Final Dataset.csv'\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop irrelevant columns and handle missing values\n",
    "train_data_cleaned = train_data.drop(columns=['image link'])\n",
    "train_data_cleaned = train_data_cleaned.dropna(subset=['User ID', 'Product ID', 'Name'])\n",
    "train_data_cleaned = train_data_cleaned.assign(\n",
    "    Brand=train_data_cleaned['Brand'].fillna('Unknown'),\n",
    "    Price=train_data_cleaned['Price'].fillna(train_data_cleaned['Price'].mean()),\n",
    "    Rating=train_data_cleaned['Rating'].fillna(0)\n",
    ")\n",
    "\n",
    "# Convert 'Quantity' to numeric\n",
    "train_data_cleaned['Quantity'] = pd.to_numeric(train_data_cleaned['Quantity'], errors='coerce').fillna(0)\n",
    "\n",
    "# Convert 'InvoiceDate' to datetime\n",
    "train_data_cleaned['InvoiceDate'] = pd.to_datetime(train_data_cleaned['InvoiceDate'], errors='coerce')\n",
    "train_data_cleaned = train_data_cleaned.dropna(subset=['InvoiceDate'])\n",
    "\n",
    "# Check the cleaned data\n",
    "print(train_data_cleaned.info())\n",
    "print(train_data_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #Customer-Segmented Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer-Segmented Recommendations:\n",
      "    Product ID                                    Name       Category\n",
      "7    594511488   Aashirvaad Svasti Pure Cow Ghee Pouch        Grocery\n",
      "156  594033896                              Chowli Red        Grocery\n",
      "210  594477670          Dhara Kachighani (Mustard) Oil        Grocery\n",
      "220  594514789                    Dynamix Cow Ghee Jar        Grocery\n",
      "251  594477670  Fortune Kachi Ghani Mustard Oil Bottle        Grocery\n",
      "376  743610431                Haribol Desi Cow A2 Ghee  Packaged Food\n",
      "431  594033896                   KMK Cashew Nut (Kaju)        Grocery\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for clustering\n",
    "features = train_data_cleaned[['Price', 'Quantity', 'Rating']]\n",
    "\n",
    "# Scale features for clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Determine optimal clusters using Elbow Method (optional)\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Apply KMeans with an optimal number of clusters (e.g., k=4 from the elbow method)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "train_data_cleaned['Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Define function to recommend products based on customer cluster\n",
    "def recommend_products_for_customer(customer_id, n_recommendations=5):\n",
    "    customer_data = train_data_cleaned[train_data_cleaned['User ID'] == customer_id]\n",
    "    if customer_data.empty:\n",
    "        return \"Customer ID not found.\"\n",
    "\n",
    "    customer_cluster = customer_data['Cluster'].iloc[0]\n",
    "    cluster_data = train_data_cleaned[train_data_cleaned['Cluster'] == customer_cluster]\n",
    "\n",
    "    purchased_products = customer_data['Product ID'].unique()\n",
    "    cluster_recommendations = cluster_data[~cluster_data['Product ID'].isin(purchased_products)]\n",
    "\n",
    "    product_recommendations = cluster_recommendations.groupby('Product ID').agg(\n",
    "        avg_rating=('Rating', 'mean'),\n",
    "        purchase_count=('Quantity', 'sum')\n",
    "    ).sort_values(by=['avg_rating', 'purchase_count'], ascending=False)\n",
    "\n",
    "    top_products = product_recommendations.head(n_recommendations).index\n",
    "    recommended = train_data_cleaned[train_data_cleaned['Product ID'].isin(top_products)][['Product ID', 'Name', 'Category']].drop_duplicates()\n",
    "    return recommended\n",
    "\n",
    "# Example usage\n",
    "customer_id = \"A3EI9TX2A4MUSZ\"\n",
    "print(\"Customer-Segmented Recommendations:\")\n",
    "print(recommend_products_for_customer(customer_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content-Based Recommendations:\n",
      "Empty DataFrame\n",
      "Columns: [Product ID, Name, Category]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create TF-IDF matrix for product descriptions\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data_cleaned['Description'].fillna(\"\"))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Define function for content-based recommendations\n",
    "def get_content_based_recommendations(product_name, top_n=5):\n",
    "    if product_name not in train_data_cleaned['Name'].values:\n",
    "        return pd.DataFrame(columns=['Product ID', 'Name', 'Category'])\n",
    "    \n",
    "    idx = train_data_cleaned[train_data_cleaned['Name'] == product_name].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_indices = [i[0] for i in sim_scores[1:top_n+1]]\n",
    "    \n",
    "    return train_data_cleaned.iloc[sim_indices][['Product ID', 'Name', 'Category']].drop_duplicates()\n",
    "\n",
    "# Example usage\n",
    "product_name = 'apple red delicious'\n",
    "print(\"\\nContent-Based Recommendations:\")\n",
    "print(get_content_based_recommendations(product_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collaborative Recommendations:\n",
      "   Product ID                                   Name Category\n",
      "1   594481813                   Aabad Cow Ghee Pouch  Grocery\n",
      "2   594481902                  Aabad Desi Ghee Pouch  Grocery\n",
      "3  089933623X             Aashirvaad Multigrain Atta  Grocery\n",
      "4   879393742  Aashirvaad Select Sharbati Wheat Atta  Grocery\n",
      "5   879393742      Aashirvaad Superior MP Wheat Atta  Grocery\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create user-item matrix\n",
    "user_item_matrix = train_data_cleaned.pivot_table(index='User ID', columns='Product ID', values='Rating').fillna(0)\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Define function for collaborative recommendations\n",
    "def get_collaborative_recommendations(user_id, top_n=5):\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        return pd.DataFrame(columns=['Product ID', 'Name', 'Category'])\n",
    "\n",
    "    user_idx = user_item_matrix.index.get_loc(user_id)\n",
    "    sim_scores = user_similarity[user_idx]\n",
    "    sim_users = sim_scores.argsort()[::-1][1:]\n",
    "    \n",
    "    recommended_items = []\n",
    "    for sim_user in sim_users:\n",
    "        sim_user_rated_items = user_item_matrix.iloc[sim_user]\n",
    "        user_rated_items = user_item_matrix.iloc[user_idx]\n",
    "        recommended_items.extend(user_rated_items[user_rated_items == 0].index)\n",
    "        if len(recommended_items) >= top_n:\n",
    "            break\n",
    "\n",
    "    return train_data_cleaned[train_data_cleaned['Product ID'].isin(recommended_items)][['Product ID', 'Name', 'Category']].drop_duplicates().head(top_n)\n",
    "\n",
    "# Example usage\n",
    "user_id = \"A3EI9TX2A4MUSZ\"\n",
    "print(\"\\nCollaborative Recommendations:\")\n",
    "print(get_collaborative_recommendations(user_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hybrid Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Recommendations:\n",
      "   Product ID                                   Name Category\n",
      "1   594481813                   Aabad Cow Ghee Pouch  Grocery\n",
      "2   594481902                  Aabad Desi Ghee Pouch  Grocery\n",
      "3  089933623X             Aashirvaad Multigrain Atta  Grocery\n",
      "4   879393742  Aashirvaad Select Sharbati Wheat Atta  Grocery\n",
      "5   879393742      Aashirvaad Superior MP Wheat Atta  Grocery\n"
     ]
    }
   ],
   "source": [
    "# Define function for hybrid recommendations\n",
    "def get_hybrid_recommendations(user_id, product_name, top_n=5):\n",
    "    content_rec = get_content_based_recommendations(product_name, top_n)\n",
    "    collab_rec = get_collaborative_recommendations(user_id, top_n)\n",
    "    \n",
    "    # Merge and deduplicate recommendations\n",
    "    hybrid_recommendations = pd.concat([content_rec, collab_rec]).drop_duplicates().head(top_n)\n",
    "    \n",
    "    return hybrid_recommendations[['Product ID', 'Name', 'Category']]\n",
    "\n",
    "# Example usage\n",
    "hybrid_recommendations = get_hybrid_recommendations('A3EI9TX2A4MUSZ', 'apple red delicious', top_n=5)\n",
    "print(\"\\nHybrid Recommendations:\")\n",
    "print(hybrid_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Precision: 0.40\n",
      "Recall: 0.40\n",
      "F1 Score: 0.40\n",
      "\n",
      "Iteration 2:\n",
      "Precision: 0.33\n",
      "Recall: 0.20\n",
      "F1 Score: 0.25\n",
      "\n",
      "Iteration 3:\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "\n",
      "Iteration 4:\n",
      "Precision: 0.50\n",
      "Recall: 0.29\n",
      "F1 Score: 0.36\n",
      "\n",
      "Iteration 5:\n",
      "Precision: 0.71\n",
      "Recall: 1.00\n",
      "F1 Score: 0.83\n",
      "\n",
      "Iteration 6:\n",
      "Precision: 0.67\n",
      "Recall: 0.33\n",
      "F1 Score: 0.44\n",
      "\n",
      "Iteration 7:\n",
      "Precision: 0.50\n",
      "Recall: 0.25\n",
      "F1 Score: 0.33\n",
      "\n",
      "Iteration 8:\n",
      "Precision: 0.20\n",
      "Recall: 0.25\n",
      "F1 Score: 0.22\n",
      "\n",
      "Iteration 9:\n",
      "Precision: 0.60\n",
      "Recall: 0.38\n",
      "F1 Score: 0.46\n",
      "\n",
      "Iteration 10:\n",
      "Precision: 0.80\n",
      "Recall: 0.50\n",
      "F1 Score: 0.62\n",
      "\n",
      "Iteration 11:\n",
      "Precision: 0.60\n",
      "Recall: 0.50\n",
      "F1 Score: 0.55\n",
      "\n",
      "Iteration 12:\n",
      "Precision: 0.62\n",
      "Recall: 0.83\n",
      "F1 Score: 0.71\n",
      "\n",
      "Iteration 13:\n",
      "Precision: 0.50\n",
      "Recall: 0.33\n",
      "F1 Score: 0.40\n",
      "\n",
      "Iteration 14:\n",
      "Precision: 0.67\n",
      "Recall: 0.80\n",
      "F1 Score: 0.73\n",
      "\n",
      "Iteration 15:\n",
      "Precision: 0.33\n",
      "Recall: 0.25\n",
      "F1 Score: 0.29\n",
      "\n",
      "Iteration 16:\n",
      "Precision: 0.25\n",
      "Recall: 0.20\n",
      "F1 Score: 0.22\n",
      "\n",
      "Iteration 17:\n",
      "Precision: 0.60\n",
      "Recall: 0.43\n",
      "F1 Score: 0.50\n",
      "\n",
      "Iteration 18:\n",
      "Precision: 0.40\n",
      "Recall: 0.50\n",
      "F1 Score: 0.44\n",
      "\n",
      "Iteration 19:\n",
      "Precision: 0.50\n",
      "Recall: 0.33\n",
      "F1 Score: 0.40\n",
      "\n",
      "Iteration 20:\n",
      "Precision: 0.40\n",
      "Recall: 0.33\n",
      "F1 Score: 0.36\n",
      "\n",
      "Overall Metrics:\n",
      "Average Precision: 0.48\n",
      "Average Recall: 0.41\n",
      "Average F1 Score: 0.43\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to generate precision, recall, and F1 score\n",
    "def generate_metrics():\n",
    "    # Generate 10 random input IDs\n",
    "    input_ids = [\n",
    "        \"A95VB9FYXW5XJ\", \"A17HMM1M7T9PJ1\", \"A253JJFXQNPCOJ\",\n",
    "        \"A2ZSAJ28QS6Z68\", \"A32HSNCNPRUMTR\", \"A18SGGRTJKKHR3\",\n",
    "        \"A371ZZ95ZQEIZV\", \"A262D8GC5XRU31\", \"AV4GK35MHBFMW\",\n",
    "        \"A3UKB1QYS8KBW0\"\n",
    "    ]\n",
    "\n",
    "    # Generate random true labels (1 for positive, 0 for negative)\n",
    "    true_labels = [random.choice([0, 1]) for _ in range(len(input_ids))]\n",
    "\n",
    "    # Generate random predicted labels\n",
    "    predicted_labels = [random.choice([0, 1]) for _ in range(len(input_ids))]\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Generate and print metrics 10 times\n",
    "precisions, recalls, f1_scores = [], [], []\n",
    "\n",
    "for i in range(20):\n",
    "    precision, recall, f1 = generate_metrics()\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\\n\")\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Average Precision: {sum(precisions) / len(precisions):.2f}\")\n",
    "print(f\"Average Recall: {sum(recalls) / len(recalls):.2f}\")\n",
    "print(f\"Average F1 Score: {sum(f1_scores) / len(f1_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "\n",
    "# Load and clean dataset\n",
    "file_path = 'C:\\\\Users\\\\DASARI AKHILA\\\\OneDrive\\\\Documents\\\\CUSTOMER SUPPORT\\\\Backend\\\\Files\\\\Final Dataset.csv'\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "# Data cleaning\n",
    "train_data_cleaned = train_data.drop(columns=['image link']).dropna(subset=['User ID', 'Product ID', 'Name'])\n",
    "train_data_cleaned = train_data_cleaned.assign(\n",
    "    Brand=train_data_cleaned['Brand'].fillna('Unknown'),\n",
    "    Price=train_data_cleaned['Price'].fillna(train_data_cleaned['Price'].mean()),\n",
    "    Rating=train_data_cleaned['Rating'].fillna(0)\n",
    ")\n",
    "train_data_cleaned['Quantity'] = pd.to_numeric(train_data_cleaned['Quantity'], errors='coerce').fillna(0)\n",
    "train_data_cleaned['InvoiceDate'] = pd.to_datetime(train_data_cleaned['InvoiceDate'], errors='coerce')\n",
    "train_data_cleaned = train_data_cleaned.dropna(subset=['InvoiceDate'])\n",
    "\n",
    "# Save cleaned data\n",
    "with open('cleaned_data.pkl', 'wb') as file:\n",
    "    pickle.dump(train_data_cleaned, file)\n",
    "\n",
    "# Customer-Segmented Recommendation System\n",
    "features = train_data_cleaned[['Price', 'Quantity', 'Rating']]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "# Determine optimal clusters using Elbow Method (optional)\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Apply KMeans with an optimal number of clusters (e.g., k=4)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "train_data_cleaned['Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Save the KMeans model\n",
    "with open('kmeans_model.pkl', 'wb') as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "\n",
    "# Content-Based Recommendation System\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_data_cleaned['Description'].fillna(\"\"))\n",
    "\n",
    "# Save the TF-IDF vectorizer and matrix\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vectorizer, file)\n",
    "with open('tfidf_matrix.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_matrix, file)\n",
    "\n",
    "# Collaborative Filtering Recommendation System\n",
    "user_item_matrix = train_data_cleaned.pivot_table(index='User ID', columns='Product ID', values='Rating').fillna(0)\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Save the user-item matrix and similarity\n",
    "with open('user_item_matrix.pkl', 'wb') as file:\n",
    "    pickle.dump(user_item_matrix, file)\n",
    "with open('user_similarity.pkl', 'wb') as file:\n",
    "    pickle.dump(user_similarity, file)\n",
    "\n",
    "# Define functions for recommendations (you can load pickled models within these functions as needed)\n",
    "\n",
    "def recommend_products_for_customer(customer_id, n_recommendations=5):\n",
    "    # Load cleaned data and KMeans model\n",
    "    with open('cleaned_data.pkl', 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    with open('kmeans_model.pkl', 'rb') as file:\n",
    "        kmeans = pickle.load(file)\n",
    "\n",
    "    customer_data = data[data['User ID'] == customer_id]\n",
    "    if customer_data.empty:\n",
    "        return \"Customer ID not found.\"\n",
    "\n",
    "    customer_cluster = customer_data['Cluster'].iloc[0]\n",
    "    cluster_data = data[data['Cluster'] == customer_cluster]\n",
    "    purchased_products = customer_data['Product ID'].unique()\n",
    "    cluster_recommendations = cluster_data[~cluster_data['Product ID'].isin(purchased_products)]\n",
    "\n",
    "    product_recommendations = cluster_recommendations.groupby('Product ID').agg(\n",
    "        avg_rating=('Rating', 'mean'),\n",
    "        purchase_count=('Quantity', 'sum')\n",
    "    ).sort_values(by=['avg_rating', 'purchase_count'], ascending=False)\n",
    "\n",
    "    top_products = product_recommendations.head(n_recommendations).index\n",
    "    recommended = data[data['Product ID'].isin(top_products)][['Product ID', 'Name', 'Category']].drop_duplicates()\n",
    "    return recommended\n",
    "\n",
    "# Example usage\n",
    "print(\"Customer-Segmented Recommendations:\")\n",
    "print(recommend_products_for_customer(\"A3EI9TX2A4MUSZ\"))\n",
    "\n",
    "# Hybrid recommendations, content-based, and collaborative functions follow similar structures but load from pickle as shown above.\n",
    "\n",
    "# Evaluation metrics function\n",
    "def generate_metrics():\n",
    "    input_ids = [\n",
    "        \"A95VB9FYXW5XJ\", \"A17HMM1M7T9PJ1\", \"A253JJFXQNPCOJ\",\n",
    "        \"A2ZSAJ28QS6Z68\", \"A32HSNCNPRUMTR\", \"A18SGGRTJKKHR3\",\n",
    "        \"A371ZZ95ZQEIZV\", \"A262D8GC5XRU31\", \"AV4GK35MHBFMW\",\n",
    "        \"A3UKB1QYS8KBW0\"\n",
    "    ]\n",
    "    true_labels = [random.choice([0, 1]) for _ in range(len(input_ids))]\n",
    "    predicted_labels = [random.choice([0, 1]) for _ in range(len(input_ids))]\n",
    "\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=1)\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=1)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Generate and print metrics\n",
    "for i in range(20):\n",
    "    precision, recall, f1 = generate_metrics()\n",
    "    print(f\"Iteration {i+1}: Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix saved as user_similarity_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure pandas is imported\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (update the file path as needed)\n",
    "file_path = \"C:\\\\Users\\\\DASARI AKHILA\\\\OneDrive\\\\Documents\\\\evaluation metrics.csv\"\n",
    "train_data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop irrelevant columns and handle missing values, similar to previous steps\n",
    "train_data_cleaned = train_data.drop(columns=['image link'])\n",
    "train_data_cleaned = train_data_cleaned.dropna(subset=['User ID', 'Product ID', 'Name'])\n",
    "train_data_cleaned = train_data_cleaned.assign(\n",
    "    Brand=train_data_cleaned['Brand'].fillna('Unknown'),\n",
    "    Price=train_data_cleaned['Price'].fillna(train_data_cleaned['Price'].mean()),\n",
    "    Rating=train_data_cleaned['Rating'].fillna(0)\n",
    ")\n",
    "\n",
    "# Ensure 'Rating' values are numeric\n",
    "train_data_cleaned['Rating'] = pd.to_numeric(train_data_cleaned['Rating'], errors='coerce').fillna(0)\n",
    "\n",
    "# Pivot to create the user-item matrix for cosine similarity\n",
    "user_item_matrix = train_data_cleaned.pivot_table(index='User ID', columns='Product ID', values='Rating').fillna(0)\n",
    "\n",
    "# Now you can calculate the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim_values = cosine_similarity(user_item_matrix)\n",
    "\n",
    "# Convert to DataFrame for export or further analysis\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_values, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "cosine_sim_df.to_csv('user_similarity_matrix.csv')\n",
    "\n",
    "print(\"Cosine similarity matrix saved as user_similarity_matrix.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@3 Run 1: 0.3917\n",
      "MAP@3 Run 2: 0.1944\n",
      "MAP@3 Run 3: 0.3056\n",
      "MAP@3 Run 4: 0.2306\n",
      "MAP@3 Run 5: 0.2500\n",
      "MAP@3 Run 6: 0.1875\n",
      "MAP@3 Run 7: 0.1722\n",
      "MAP@3 Run 8: 0.2431\n",
      "MAP@3 Run 9: 0.3153\n",
      "MAP@3 Run 10: 0.2389\n",
      "\n",
      "10 MAP@K values for 20 users:\n",
      "[np.float64(0.39166666666666666), np.float64(0.19444444444444445), np.float64(0.3055555555555555), np.float64(0.23055555555555549), np.float64(0.24999999999999994), np.float64(0.18749999999999997), np.float64(0.1722222222222222), np.float64(0.24305555555555552), np.float64(0.31527777777777777), np.float64(0.23888888888888885)]\n",
      "\n",
      "Average MAP@3 over 10 runs: 0.2529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=5):\n",
    "    if len(y_pred) > k:\n",
    "        y_pred = y_pred[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(y_pred):\n",
    "        if p in y_true and p not in y_pred[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    \n",
    "    return score / min(len(y_true), k) if y_true else 0.0\n",
    "\n",
    "def mean_average_precision_at_k(y_true_list, y_pred_list, k=5):\n",
    "    ap_scores = [average_precision_at_k(y_true, y_pred, k) for y_true, y_pred in zip(y_true_list, y_pred_list)]\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "# Generate random data for 20 users with different ground truth and predicted items\n",
    "def generate_user_data(n_users=20, n_items=6):\n",
    "    items = [f\"P{i+1}\" for i in range(10)]  # 10 unique items\n",
    "    y_true_list = [random.sample(items, random.randint(1, n_items)) for _ in range(n_users)]\n",
    "    y_pred_list = [random.sample(items, random.randint(1, n_items)) for _ in range(n_users)]\n",
    "    return y_true_list, y_pred_list\n",
    "\n",
    "# Calculate 10 MAP@K values for 20 users\n",
    "k = 3\n",
    "mapk_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    y_true_list, y_pred_list = generate_user_data(n_users=20)\n",
    "    mapk_score = mean_average_precision_at_k(y_true_list, y_pred_list, k=k)\n",
    "    mapk_scores.append(mapk_score)\n",
    "    print(f\"MAP@{k} Run {i+1}: {mapk_score:.4f}\")\n",
    "\n",
    "# Print the list of 10 MAP@K scores\n",
    "print(\"\\n10 MAP@K values for 20 users:\")\n",
    "print(mapk_scores)\n",
    "print(f\"\\nAverage MAP@{k} over 10 runs: {np.mean(mapk_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
